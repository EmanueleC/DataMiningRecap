\section{Classificazione}

In questo caso la response variabile Y è chiamata \textbf{categorica} ed è
qualitativa.

Predirre una variabile categorica ha lo scopo di assegnare una osservazione
a una categoria/classe.

Esempio: predirre se si verifichera un evento climatico disastroso date alcune
condizioni come temperatura, pressione, vento ecc\dots

Nel caso di una response variable binaria non è possibile usare il metodo di
regressione lineare poichè i valori che assume non sono reali ma discreti (0 o
1).

Una ``soluzione'' consiste nell'usare una funzione che restituisca valori reali
solamente nell'intervallo [0,1]. Il modello comunemente utilizzato per questo
scopo è il \textbf{modello di regressione logistica}.

\begin{equation}
P(Y=1|X) = \frac{e^{\beta_0+\beta_1 X}}{1 + e^{\beta_0+\beta_1 X}}
\end{equation}

Questa funzione assume valori in [0,1] indipendentemente da $\beta_0$ e
$\beta_1$.

Gli \textbf{odds} sono definiti dalla funzione:

\begin{equation}
\frac{P(Y=1|X)}{1-P(Y=1|X)} = e^{\beta_0+\beta_1 X}
\end{equation}

Valori vicini a 0 $\rightarrow$ probabilità molto bassa che Y=1.

Valori vicini a 1 $\rightarrow$ probabilità molto alta che Y=1.

I \textbf{logit} o \textbf{logodds} applicano il logaritmo agli odds:

\begin{equation}
\log \left( \frac{P(Y=1|X)}{1-P(Y=1|X)} \right) = \beta_0+\beta_1 X
\end{equation}

Questo termine \textbf{logit} è lineare in X.

\subsection{Stima dei parametri}

Le stime $\hat{\beta_0}$ e $\hat{\beta_1}$ sono valori che minimizzano
la seguente funzione di likelihood:

\begin{equation}
L(\beta_0, \beta_1) = \prod_{i:y_i = 1} P(y_i=1|x_i)
\prod_{j:y_j = 0} \{1 - P(y_j=1|x_j)\}
\end{equation}

\subsection{Accuratezza di un modello di regressione logistica}

Un concetto simile a $R^2$ per la regressione lineare, in questo caso è la
devianza.

Data la più grande likelihood $L_max$ del modello saturato, e data la
likelihood del modello corrente, $L_mod$, la devianza è:

$dev = 2(log L_max - log L_mod)$.

Se il modello è buono, allora $Dev \sim X^2_{n-p-1}$, altrimenti se il valore
di Dev è più grande non è soddisfacente.

Si consideri un modello $M_1$ con $p_1$ covarianti e un modello $M_2$ con
$p_2$ covarianti, più semplici di $M_1$.

La differenza tra le devianze dei modelli dà un'indicazione sulla semplificazione
effettuata: se segue una distribuzione $X^2_{p_1 - p_2}$ allora la semplificazione
è legittima; altrimenti il modello $M_2$ non è giustificabile.








